<!-- responsible for this page is benjamin.blankertz@tu-berlin.de -->
<!DOCTYPE html PUBLIC "-//w3c//dtd html 4.01 transitional//en">
<html><head>
  <meta http-equiv="content-type" content="text/html; charset=UTF-8">
  <title>Data Set 1 for the BCI Competition IV</title>
  <style type="text/css">
    body { margin-left:5%; margin-right:5% }
  </style>
</head>
<body>

<font face="sans-serif">

<h2 align="center">
Data sets 1
<i>‹motor imagery, uncued classifier application›</i>
</h2>
<p></p>

<p>Data sets 
provided by the 
<a href="http://www.bbci.de/">Berlin BCI</a>
group:
Berlin Institute of Technology 
(<a href="http://ml.cs.tu-berlin.de/en/index.html">Machine Learning Laboratory</a>) and
Fraunhofer FIRST (<a href="http://www.first.fhg.de/ida/">Intelligent Data Analysis Group</a>)
(<a href="http://ml.cs.tu-berlin.de/en/klaus/index.html">Klaus-Robert Müller</a>, 
<a href="http://ml.cs.tu-berlin.de/~blanker">Benjamin Blankertz</a>, Carmen Vidaurre, <a href="http://ida.first.fraunhofer.de/~nolte/">Guido Nolte</a>), 
and Campus Benjamin Franklin of the Charité - University Medicine 
Berlin, Department of Neurology, Neurophysics Group (Gabriel Curio)
</p>

<p>
Correspondence to Benjamin Blankertz 
<a href="mailto:benjamin.blankertz@tu-berlin.de">
‹benjamin.blankertz@tu-berlin.de›</a>
</p>


<p></p>
<h3>The Thrill</h3>
Most demonstrations of algorithms on BCI data are evaluating
classification of EEG trials, i.e., windowed EEG signals for fixed
length, where each trial corresponds to a specific mental state.
But in BCI applications with asynchronous feedback one is faced with 
the problem that the classifier has to be applied continuously to 
the incoming EEG without having cues of when the subject is
switching her/his intention. This data set poses the challenge of
applying a classifier to continuous EEG for which no cue information
is given. <br>

Another issue that is addressed in this data set is that the
evaluation data contains periods in which the user has no control
intention. During those intervals the classifier is supposed to return
0 (no affiliation to one of the target classes).


<p></p>
<h3>Experimental Setup</h3>
These data sets were recorded from healthy subjects. In the whole
session motor imagery was performed without feedback. For each 
subject two classes of motor imagery were selected from the three
classes <i>left hand</i>, <i>right hand</i>, and
<i>foot</i> (side chosen by the subject; optionally also both feet).
<br>

<b>Calibration data:</b> In the first two runs, arrows pointing left,
right, or down were presented as visual cues on a computer
screen. Cues were displayed for a period of 4s during which the
subject was instructed to perform the cued motor imagery task. These
periods were interleaved with 2s of blank screen and 2s with a
fixation cross shown in the center of the screen.  The fixation cross
was superimposed on the cues, i.e. it was shown for 6s. These data
sets are provided with complete marker information.
<br>

<b>Evaluation data:</b> Then 4 runs followed which are used for
evaluating the submissions to the competitions. Here, the motor
imagery tasks were cued by soft acoustic stimuli (words <i>left</i>,
<i>right</i>, and <i>foot</i>) for periods of varying length between
1.5 and 8 seconds. The end of the motor imagery period was indicated
by the word <i>stop</i>.  Intermitting periods had also a varying
duration of 1.5 to 8s.  Note that in the evaluation data, there are
not necessarily equally many trials from each condition.  <br>

<b>Special Feature:</b>
Some of the data sets were <i>artificially generated</i>. The idea
is to have a means for generating artifical EEG signals with
specified properties that is such realistic that it can be used to
evaluate and compare analysis techniques. The outcome of the 
competition will show whether the applied methods perform comparably
on artifical and real data. The only information we provide is that
there is at least one real and at least one artificial data set,
while the true distribution remains undisclosed until the submission
deadline. For competition purpose, only results for the real
data set(s) are considered.
The functions for generating artificial data were provided by
<a href="http://ida.first.fraunhofer.de/~nolte/">Guido Nolte</a> and Carmen Vidaurre.


<p></p>
<h3>Format of the Data</h3>

<p>Given are continuous signals of 59 EEG channels and, for the
calibration data, markers that indicate the time points of cue 
presentation and the corresponding target classes. 
</p>

<p>
Data are provided in <b>Matlab</b> format (<tt>*.mat</tt>) containing 
variables:
</p><ul>
 <li><tt>cnt</tt>: the continuous EEG signals, size [time x channels].
  The array is stored in datatype <tt>INT16</tt>. To convert it to
  uV values, use <tt>cnt= 0.1*double(cnt);</tt> in Matlab. </li>
 <li><tt>mrk</tt>: structure of target cue information with fields
  (the file of evaluation data does <i>not</i> contain this variable)
 <ul>
  <li><tt>pos</tt>: vector of positions of the cue in the EEG signals given in
   unit <i>sample</i>, length #cues </li>
  <li><tt>y</tt>: vector of target classes (-1 for class one or 1 for
   class two), length #cues </li> 
 </ul>
 </li><li><tt>nfo</tt>: structure providing additional information with fields
 <ul>
  <li><tt>fs</tt>: sampling rate, </li>
  <li><tt>clab</tt>: cell array of channel labels, </li>
  <li><tt>classes</tt>: cell array of the names of the motor imagery classes, </li>
  <li><tt>xpos</tt>: x-position of electrodes in a 2d-projection, </li>
  <li><tt>ypos</tt>: y-position of electrodes in a 2d-projection. </li> 
 </ul>
</li></ul>

As alternative, data is also provided in zipped <b>ASC II</b> format:
<ul>
 <li><tt>*_cnt.txt</tt>: the continuous EEG signals, where each
  row holds the values for all channels at a specific time point </li>
 <li><tt>*_mrk.txt</tt>: target cue information, each row represents one cue
  where the first value defines the time point (given in unit <i>sample</i>),
  and the second value the target class (-1 for class one or 1 for
  class two). For evaluation data <i>no</i> <tt>*_mrk.txt</tt> file is provided.
  </li>
 <li><tt>*_nfo.txt</tt>: contains other information as described for the
  matlab format. </li>
</ul> 
<p></p>


<p></p>
<h3>Requirements and Evaluation</h3>

<p>
Please provide an ASC II file (named 'Result_BCIC_IV_ds1.txt') containing
classifier outputs (real number between -1 and 1) for each sample
point of the evaluation signals, one value per line. 
The submissions are evaluated in view of a one
dimensional cursor control application with range from -1 to 1. The
mental state of class one is used to position the cursor at -1, and the
mental state of class two is used to position the cursor near 1. In the
absense of those mental states (intermitting intervals) the
cursor should be at position 0.  Note that it is unknown to the
competitors at what intervals the subject is in a defined mental
state.  Competitiors submit classifier outputs for all time points.
The evaluation function calculates the squared error with respect to
the target vector that is -1 for class one, 1 for class two,
and 0 otherwise, averaged across time points. In the averaging we
will ignore time points during transient periods (1s starting from
each cue). For competition purpose, only results for the real
data set(s) are considered, but results for artifical data are
also reported for comparison.<br>

Optionally, please report which of the data sets you think to be
artificially generated. <br>

You also have to provide a description of the used algorithm (ASC II,
HTML or PDF format) for publication at the results web page.
</p>



<p></p> 
<h3>Technical Information</h3> 

The recording was made using BrainAmp MR plus amplifiers and a Ag/AgCl
electrode cap. Signals from 59 EEG positions were measured that were
most densely distributed over sensorimotor areas.   Signals were
band-pass filtered between 0.05 and 200&nbsp;Hz and then digitized at
1000&nbsp;Hz with 16 bit (0.1 uV) accuracy.  We provide also a version
of the data that is downsampled at 100&nbsp;Hz (first low-pass
filtering the original data (Chebyshev Type II filter of order 10 with
stopband ripple 50dB down and stopband edge frequency 49Hz) and then 
calculating the mean of blocks of 10 samples).


<p></p> 
<h3>References</h3>
Any publication that analyzes this data set should cite the following
paper as a reference of the recording:

<li><a name="bladorkramuecur07"></a><strong>Benjamin Blankertz, Guido Dornhege, Matthias Krauledat,
  Klaus-Robert Müller, and Gabriel Curio</strong>.
<a href="http://dx.doi.org/10.1016/j.neuroimage.2007.01.051">The non-invasive
  Berlin Brain-Computer Interface: Fast acquisition of effective
  performance in untrained subjects</a>.
<cite>NeuroImage</cite>, 37(2):539-550, 2007.
[<a href="http://ml.cs.tu-berlin.de/publications/BlaDorKraMueCur07.pdf">pdf</a>]

<p></p>
<hr>

<p align="center">
[ <a href="http://www.bbci.de/competition/iv">BCI Competition IV</a> ]
</p><p>



</p></li></font></body></html>